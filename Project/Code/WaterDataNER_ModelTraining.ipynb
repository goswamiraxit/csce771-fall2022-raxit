{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generate annotated data in Conll format"
      ],
      "metadata": {
        "id": "bO7n66QvgH0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "import os\n",
        "# docs used in annotations\n",
        "txt_dir = '/content/drive/MyDrive/CSCE Fall2022-Raxit/Course Project/Experiment_Validated_Annotation/Mississippi/PDF_PAGES_INTO_TXTFILES_WITH_LAYOUT/'\n",
        "def get_docnames():\n",
        "    dir_list= os.listdir(txt_dir)\n",
        "    #doc_2_annotation = [{doc_2_annotation[x] = []} \n",
        "    doc_2_annotation = OrderedDict()\n",
        "    for x in dir_list:\n",
        "        doc_2_annotation[x] = []\n",
        "    return doc_2_annotation\n",
        "\n",
        "doc_2_annotation = get_docnames()"
      ],
      "metadata": {
        "id": "4ss7iacPgGgN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bean class to hold annotation attributes\n",
        "class Annotation():\n",
        "    def __init__(self, doc_name, begin, end, entity_text, entity_type, add_to_conll=False):\n",
        "        self.doc_name = doc_name\n",
        "        self.begin = begin\n",
        "        self.end = end\n",
        "        self.entity_text = entity_text\n",
        "        self.entity_type = entity_type\n",
        "        self.add_to_conll = add_to_conll\n",
        "    def __str__(self):\n",
        "        return self.doc_name + ' ' + str(self.begin) + ' ' + str(self.end) + ' ' + self.entity_text + ' ' + self.entity_type\n",
        "    def __eq__(self,other):\n",
        "        return self.begin == other.begin and self.end == other.end \\\n",
        "                and self.entity_text == other.entity_text and self.entity_type == other.entity_type"
      ],
      "metadata": {
        "id": "pgfGpYzD49Jv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_annotations(annotation_file_name, doc_2_annotation):\n",
        "    annotation_list = []\n",
        "    with open(annotation_file_name) as reader:\n",
        "        for line in reader:\n",
        "            res = line.split('\\t')\n",
        "            #print(res)\n",
        "            annotation = Annotation(res[4]+\".txt\", int(res[2]), int(res[3]), [1], \"CHEMICAL\")\n",
        "            docname = res[4]+\".txt\"\n",
        "            #print(docname)\n",
        "            if docname not in doc_2_annotation.keys():\n",
        "                continue\n",
        "            doc_2_annotation[docname].append(annotation)\n",
        "            annotation_list.append(annotation)\n",
        "    return doc_2_annotation, annotation_list"
      ],
      "metadata": {
        "id": "G8bGTMeu4998"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_type(annotation_list, begin, end):\n",
        "    if end - begin == 1:\n",
        "        return \"O\"\n",
        "    for antn in annotation_list:\n",
        "        antn_type = antn.entity_type\n",
        "        antn_begin = antn.begin\n",
        "        antn_end = antn.end\n",
        "        if begin <= antn_begin and antn_begin < end:\n",
        "            antn.add_to_conll = True\n",
        "            return antn_type\n",
        "            #return 'B-'+antn_type\n",
        "        elif (antn_begin < begin and begin < antn_end) or (antn_begin < end  and end <= antn_end):\n",
        "            #antn.add_to_conll = True\n",
        "            return antn_type\n",
        "    return \"O\""
      ],
      "metadata": {
        "id": "2SJ4PB_95CyK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_overlap_anotn(annotation_list):\n",
        "    #clean_annoation_list = []\n",
        "    flag = False\n",
        "    for x in annotation_list:\n",
        "        for y in annotation_list:\n",
        "            if x != y:\n",
        "                #check if y is a part of x\n",
        "                if x.begin <= y.begin and y.end < x.end:\n",
        "                    print(\"Overlap: \")\n",
        "                    print(str(y) + \"is a part of \\n\" + str(x))\n",
        "                    annotation_list.remove(y)\n",
        "    return annotation_list  \n",
        "\n",
        "def check_annotation_not_added_in_Conll(annotation_list):\n",
        "    for x in annotation_list:\n",
        "        if not x.add_to_conll:\n",
        "            print(str(x))"
      ],
      "metadata": {
        "id": "n9oSDXle5Iv8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import re\n",
        "#import nltk\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n",
        "# from spacy.pipeline import Sentencizer\n",
        "# sentencizer = Sentencizer()\n",
        "config = {\"punct_chars\": None}\n",
        "nlp.add_pipe(\"sentencizer\", config=config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fz4DpJYd5L0_",
        "outputId": "3317c9fe-79fa-44ad-8d0f-28a584667717"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/language.py:1899: UserWarning: [W123] Argument disable with value ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer'] is used instead of ['senter'] as specified in the config. Be aware that this might affect other components in your pipeline.\n",
            "  config_value=config[\"nlp\"][key],\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.pipeline.sentencizer.Sentencizer at 0x7f014937b2d0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate TSV version of annotation\n",
        "import re\n",
        "list_str = \"\"\n",
        "for line in open(\"list.txt\").readlines():\n",
        "    line = line.strip().lower()\n",
        "    if len(line) == 0:\n",
        "        continue\n",
        "    list_str+=re.escape(line)+\"|\"+re.escape(line+\"s\")+\"|\"\n",
        "    \n",
        "list_str = list_str[:-1]\n",
        "\n",
        "regex = \"\\\\b(\"+list_str+\")\\\\b\"\n",
        "#print(regex)\n",
        "\n",
        "def custom_tokenizer(token):\n",
        "    actual_token = token\n",
        "    #print(actual_token)\n",
        "    token = token.lower()\n",
        "    final_tokens = []\n",
        "    match_beg_end_list = []\n",
        "    for i, match in enumerate(re.finditer(regex, token)):\n",
        "        #print(i, match.group())\n",
        "        beg = match.span()[0]\n",
        "        end = match.span()[1]\n",
        "        match_beg_end_list.append((beg, end))\n",
        "    start = 0\n",
        "    end = len(actual_token)\n",
        "    for i, x in enumerate(match_beg_end_list):\n",
        "        if start != x[0]:\n",
        "            final_tokens.append(actual_token[start:x[0]])\n",
        "        final_tokens.append(actual_token[x[0]:x[1]])\n",
        "        start = x[1]\n",
        "    if len(actual_token[end:]) > 0:\n",
        "        final_tokens.append(actual_token[end:])\n",
        "    if len(final_tokens) == 0:\n",
        "        final_tokens.append(actual_token)\n",
        "    return final_tokens\n",
        "\n",
        "print(custom_tokenizer(\"C.F.R.\"))"
      ],
      "metadata": {
        "id": "Kx1PLUyV5Rs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "annotation_file_name = \"annotation_validated.tsv\"\n",
        "doc_2_annotation, annotation_list = read_annotations(annotation_file_name, doc_2_annotation)\n",
        "# output connll file\n",
        "out_conll_path = \"annotation.conll\"\n",
        "writer = open(out_conll_path, 'w')\n",
        "flag = False\n",
        "annotation_counter = 0\n",
        "entity_mismatch = 0\n",
        "entity_split_by_newline_counter = 0\n",
        "NON_ENTITY_DOC  = 0\n",
        "print(doc_2_annotation.keys())\n",
        "for doc_name in doc_2_annotation.keys():\n",
        "    annotation_list = doc_2_annotation[doc_name]\n",
        "    if len(annotation_list) == 0:\n",
        "        NON_ENTITY_DOC +=1\n",
        "        continue\n",
        "    annotation_list = remove_overlap_anotn(annotation_list)\n",
        "    annotation_counter = annotation_counter + len(annotation_list)\n",
        "    doc = open(txt_dir+doc_name).read() \n",
        "    #print(doc_name, doc)\n",
        "    entity_idx = 0\n",
        "    prev_tag = ''\n",
        "    offset = 0\n",
        "    for paragraph in re.split(\"[\\n\\r]\", doc):\n",
        "        sentences = nlp(paragraph)\n",
        "        for sentence in sentences.sents:\n",
        "            if sentence.text:\n",
        "                tokens_ = nlp(sentence.text)\n",
        "                for tok in tokens_:\n",
        "                    tokens = custom_tokenizer(tok.text)\n",
        "                    for token in tokens:\n",
        "                        offset = doc.find(token, offset)\n",
        "                        begin = offset\n",
        "                        end = offset + len(token)\n",
        "                        offset += len(token)\n",
        "                        if begin == -1:\n",
        "                            print(\"Begin starts with -1\")\n",
        "                            print(sentence.text)\n",
        "                            print(str(token) + '\\t' + str(begin) + '\\t' + str(end)  + '\\t' + doc_name+'\\t'+entity_type+'\\n')\n",
        "                            print(\"#################################################################\")\n",
        "                        entity_type = get_type(annotation_list, begin, end)\n",
        "                        writer.write(str(token) + '\\t' + str(begin) + '\\t' + str(end)  + '\\t' + doc_name+'\\t'+entity_type+'\\n')\n",
        "                writer.write('\\n')\n",
        "writer.close()\n",
        "print('# of annotations in conll: ', annotation_counter)\n",
        "print('Entity Mismatch: ', entity_mismatch)\n",
        "print('Entity Split by new line: ', entity_split_by_newline_counter)\n",
        "print(\"Non enitity docs: \", NON_ENTITY_DOC)"
      ],
      "metadata": {
        "id": "qdSkQwzx55Us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPURvM41vH-4"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!gdown --id 13758oOJgPmqEQITQpdT2cRaXNHIBCQoU \n",
        "!gdown --id 13758oOJgPmqEQITQpdT2cRaXNHIBCQoU\n",
        "!unzip 5-fold.zip\n",
        "\n",
        "#https://drive.google.com/file/d/13758oOJgPmqEQITQpdT2cRaXNHIBCQoU/view?usp=sharing"
      ],
      "metadata": {
        "id": "FvhDCxtMwFOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 5 \"5-fold/testData_1\".txt"
      ],
      "metadata": {
        "id": "jHzM8BVfwRmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "def generate_datasets():\n",
        "    for i in range(1, 6):\n",
        "        print(i)\n",
        "        writer = open(f'/content/train{i}.json', 'w')\n",
        "        words = []\n",
        "        ner = []\n",
        "        with open(f'/content/5-fold/trainData_{i}.txt') as reader:\n",
        "            for line in reader:\n",
        "                items = line.strip().split('\\t')\n",
        "                #print(items)\n",
        "                if len(items) < 3 and len(words) > 1:\n",
        "                    writer.write(json.dumps({\"words\":words, \"ner_tags\": ner}))\n",
        "                    writer.write('\\n')\n",
        "                    words = []\n",
        "                    ner = []\n",
        "                    continue\n",
        "                if len(items) > 3:\n",
        "                    words.append(items[0])\n",
        "                    ner.append(items[-1])\n",
        "\n",
        "        writer = open(f'/content/test{i}.json', 'w')\n",
        "        words = []\n",
        "        ner = []\n",
        "        with open(f'/content/5-fold/testData_{i}.txt') as reader:\n",
        "            for line in reader:\n",
        "                items = line.strip().split('\\t')\n",
        "                if len(items) < 3 and len(words) > 1:\n",
        "                    writer.write(json.dumps({\"words\":words, \"ner_tags\": ner}))\n",
        "                    writer.write('\\n')\n",
        "                    words = []\n",
        "                    ner = []\n",
        "                    continue\n",
        "                if len(items) > 3:\n",
        "                    words.append(items[0])\n",
        "                    ner.append(items[-1])\n",
        "\n",
        "generate_datasets()"
      ],
      "metadata": {
        "id": "-WMUTZyRwrC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Training"
      ],
      "metadata": {
        "id": "v9hm0Hdx_A4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/huggingface/transformers\n",
        "%cd transformers\n",
        "!pip install .\n",
        "!pip install -r /content/transformers/examples/pytorch/token-classification/requirements.txt\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "BsfyrUnFw3TS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FOLD = 1\n",
        "import os\n",
        "os.makedirs(f\"/content/drive/MyDrive/CSCE Fall2022-Raxit/Course Project/Experiment_Validated_Annotation/Mississippi/Results/bert-base-cased/checkpoint1\", exist_ok=True)\n",
        "%cd /content/transformers/examples/pytorch/token-classification\n",
        "!python run_ner.py \\\n",
        "  --model_name_or_path \"/content/drive/MyDrive/CSCE Fall2022-Raxit/Course Project/WaterBERT/water-bert-cased\" \\\n",
        "  --train_file /content/train1.json \\\n",
        "  --validation_file  /content/test1.json \\\n",
        "  --test_file  /content/test1.json \\\n",
        "  --output_dir \"/content/drive/MyDrive/CSCE Fall2022-Raxit/Course Project/Experiment_Validated_Annotation/Mississippi/Results/water-bert-cased/checkpoint1\" \\\n",
        "  --per_device_train_batch_size 16 \\\n",
        "  --per_device_eval_batch_size 16 \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --do_predict \\\n",
        "  --num_train_epochs 3\\\n",
        "  --evaluation_strategy epoch\\\n",
        "  --overwrite_output_dir True\\\n",
        "  --save_steps=10000"
      ],
      "metadata": {
        "id": "_PvJ02JXxMtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FOLD = 2\n",
        "import os\n",
        "os.makedirs(f\"/content/drive/MyDrive/CSCE Fall2022-Raxit/Course Project/Experiment_Validated_Annotation/Mississippi/Results/water-bert-cased/checkpoint2\", exist_ok=True)\n",
        "%cd /content/transformers/examples/pytorch/token-classification\n",
        "!python run_ner.py \\\n",
        "  --model_name_or_path \"/content/drive/MyDrive/CSCE Fall2022-Raxit/Course Project/WaterBERT/water-bert-cased\" \\\n",
        "  --train_file /content/train2.json \\\n",
        "  --validation_file  /content/test2.json \\\n",
        "  --test_file  /content/test2.json \\\n",
        "  --output_dir \"/content/drive/MyDrive/CSCE Fall2022-Raxit/Course Project/Experiment_Validated_Annotation/Mississippi/Results/water-bert-cased/checkpoint2\" \\\n",
        "  --per_device_train_batch_size 16 \\\n",
        "  --per_device_eval_batch_size 16 \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --do_predict \\\n",
        "  --num_train_epochs 3\\\n",
        "  --evaluation_strategy epoch\\\n",
        "  --overwrite_output_dir True\\\n",
        "  --save_steps=10000"
      ],
      "metadata": {
        "id": "Y6LWmRq9OwTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FOLD = 3\n",
        "import os\n",
        "os.makedirs(f\"/content/drive/MyDrive/CSCE Fall2022-Raxit/Course Project/Experiment_Validated_Annotation/Mississippi/Results/water-bert-cased/checkpoint3\", exist_ok=True)\n",
        "%cd /content/transformers/examples/pytorch/token-classification\n",
        "!python run_ner.py \\\n",
        "  --model_name_or_path \"/content/drive/MyDrive/CSCE Fall2022-Raxit/Course Project/WaterBERT/water-bert-cased\" \\\n",
        "  --train_file /content/train3.json \\\n",
        "  --validation_file  /content/test3.json \\\n",
        "  --test_file  /content/test3.json \\\n",
        "  --output_dir \"/content/drive/MyDrive/CSCE Fall2022-Raxit/Course Project/Experiment_Validated_Annotation/Mississippi/Results/water-bert-cased/checkpoint3\" \\\n",
        "  --per_device_train_batch_size 16 \\\n",
        "  --per_device_eval_batch_size 16 \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --do_predict \\\n",
        "  --num_train_epochs 3\\\n",
        "  --evaluation_strategy epoch\\\n",
        "  --overwrite_output_dir True\\\n",
        "  --save_steps=10000"
      ],
      "metadata": {
        "id": "UbGXQKbVPD-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FOLD = 4\n",
        "import os\n",
        "os.makedirs(f\"/content/drive/MyDrive/CSCE Fall2022-Raxit/Course Project/Experiment_Validated_Annotation/Mississippi/Results/water-bert-cased/checkpoint4\", exist_ok=True)\n",
        "%cd /content/transformers/examples/pytorch/token-classification\n",
        "!python run_ner.py \\\n",
        "  --model_name_or_path \"/content/drive/MyDrive/CSCE Fall2022-Raxit/Course Project/WaterBERT/water-bert-cased\" \\\n",
        "  --train_file /content/train4.json \\\n",
        "  --validation_file  /content/test4.json \\\n",
        "  --test_file  /content/test4.json \\\n",
        "  --output_dir \"/content/drive/MyDrive/CSCE Fall2022-Raxit/Course Project/Experiment_Validated_Annotation/Mississippi/Results/water-bert-cased/checkpoint4\" \\\n",
        "  --per_device_train_batch_size 16 \\\n",
        "  --per_device_eval_batch_size 16 \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --do_predict \\\n",
        "  --num_train_epochs 3\\\n",
        "  --evaluation_strategy epoch\\\n",
        "  --overwrite_output_dir True\\\n",
        "  --save_steps=10000"
      ],
      "metadata": {
        "id": "3aPbnKzPxU6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FOLD = 5\n",
        "import os\n",
        "os.makedirs(f\"/content/drive/MyDrive/CSCE Fall2022-Raxit/Course Project/Experiment_Validated_Annotation/Mississippi/Results/water-bert-cased/checkpoint5\", exist_ok=True)\n",
        "%cd /content/transformers/examples/pytorch/token-classification\n",
        "!python run_ner.py \\\n",
        "  --model_name_or_path \"/content/drive/MyDrive/CSCE Fall2022-Raxit/Course Project/WaterBERT/water-bert-cased\" \\\n",
        "  --train_file /content/train5.json \\\n",
        "  --validation_file  /content/test5.json \\\n",
        "  --test_file  /content/test5.json \\\n",
        "  --output_dir \"/content/drive/MyDrive/CSCE Fall2022-Raxit/Course Project/Experiment_Validated_Annotation/Mississippi/Results/water-bert-cased/checkpoint5\" \\\n",
        "  --per_device_train_batch_size 16 \\\n",
        "  --per_device_eval_batch_size 16 \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --do_predict \\\n",
        "  --num_train_epochs 3\\\n",
        "  --evaluation_strategy epoch\\\n",
        "  --overwrite_output_dir True\\\n",
        "  --save_steps=10000"
      ],
      "metadata": {
        "id": "UrMoZuIzPFXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#scibert-scivocab-uncased\n",
        "#chemical-bert-uncased\n",
        "#dmis-lab/biobert-base-cased-v1.2"
      ],
      "metadata": {
        "id": "AZVCxS8VVpra"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}